diff -uprN SAiD/compare_result.py SAiD_river/SAiD/compare_result.py
--- SAiD/compare_result.py	1970-01-01 08:00:00.000000000 +0800
+++ SAiD_river/SAiD/compare_result.py	2025-06-17 12:55:28.094225049 +0800
@@ -0,0 +1,24 @@
+import sys
+import torch
+
+ref_name=sys.argv[1]
+cur_name=sys.argv[2]
+
+ref_tensor = torch.load(ref_name)
+cur_tensor = torch.load(cur_name)
+
+print(ref_tensor.shape, ref_tensor.numel())
+print(torch.allclose(ref_tensor, cur_tensor,atol=1e-2))
+torch.set_printoptions(sci_mode=False)
+
+equality = torch.eq(ref_tensor, cur_tensor)
+# Find indices where tensors are not equal
+not_equal_indices = torch.where(equality == False)
+# Extract and print the differing elements
+differing_elements_1 = ref_tensor[not_equal_indices]
+differing_elements_2 = cur_tensor[not_equal_indices]
+print(f"Differing elements in tensor1: {differing_elements_1}")
+print(f"Differing elements in tensor2: {differing_elements_2}")
+print(f"Differing elements in tensor2: {differing_elements_2 - differing_elements_1}")
+
+print("diff size: ",differing_elements_1.shape)
diff -uprN SAiD/README.md SAiD_river/SAiD/README.md
--- SAiD/README.md	2025-06-17 12:48:01.043195035 +0800
+++ SAiD_river/SAiD/README.md	2025-06-17 12:55:28.094225049 +0800
@@ -46,6 +46,60 @@ python script/inference.py \
         [--mask_path "<input_mask>.csv"]  # Required for editing
 ```
 
+## Enable OpenVINO pipeline
+
+Install OpenVINO package
+
+```bash
+   python3 -m pip install openvino-dev[tensorflow2,onnx]
+```
+
+Convert torch file to be IR file
+
+```bash
+python3 script/inference.py \
+    --weights_path "<SAiD>.pth" \
+    --audio_path "<input_audio>.wav" \
+    --output_path "<output_coeffs>.csv" \
+    --device cpu \
+    --dynamic_shape True \
+    --convert_model True \
+    --ov_model_path "<ov_model_path>" \
+    --num_steps 1
+```
+
+Then you will see generated IR files in <ov_model_path>:
+```
+-rw-rw-r-- 1 openvino-ci-74 openvino-ci-74 188741872  7月 26 18:04 ModifiedWav2Vec2Model.bin
+-rw-rw-r-- 1 openvino-ci-74 openvino-ci-74    532793  7月 26 18:04 ModifiedWav2Vec2Model.xml
+-rw-rw-r-- 1 openvino-ci-74 openvino-ci-74  13986042  7月 26 18:04 UNet1DConditionModel.bin
+-rw-rw-r-- 1 openvino-ci-74 openvino-ci-74    552604  7月 26 18:04 UNet1DConditionModel.xml
+```
+
+Run inference with OpenVINO
+
+```bash
+python3 script/inference.py \
+    --weights_path "<SAiD>.pth" \
+    --audio_path "<input_audio>.wav" \
+    --output_path "<output_coeffs>.csv" \
+    --device gpu.1 \
+    --use_ov True \
+    --ov_model_path "<ov_model_path>" \
+    --num_steps 200
+
+ov::compiled_model:  /home/openvino-ci-74/river/SAiD/dynamic_models_1/UNet1DConditionModel.xml, device_name =  GPU.1  ...done
+input: [<ConstOutput: names[sample] shape[2,?,32] type: f32>, <ConstOutput: names[56, timestep] shape[2] type: i64>, <ConstOutput: names[encoder_hidden_states] shape[2,?,768] type: f32>, <ConstOutput: names[bias_mask] shape[2,?,?] type: boolean>]
+output: [<ConstOutput: names[] shape[2,1..,32] type: f32>]
+
+100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 241.34it/s]
+Steps = 200, device = GPU.1
+Inference latency: 926.8906116485596 ms
+
+```
+
+
+
 ## BlendVOCA
 
 ### Construct Blendshape Facial Model
diff -uprN SAiD/run_export.sh SAiD_river/SAiD/run_export.sh
--- SAiD/run_export.sh	1970-01-01 08:00:00.000000000 +0800
+++ SAiD_river/SAiD/run_export.sh	2025-06-17 12:55:28.094225049 +0800
@@ -0,0 +1,17 @@
+export HF_ENDPOINT=https://hf-mirror.com
+
+python3 script/inference.py \
+    --weights_path "/home/openvino-ci-74/river/SAiD/SAiD/model/SAiD.pth" \
+    --audio_path "/home/openvino-ci-74/river/SAiD/audio1.wav" \
+    --output_path "/home/openvino-ci-74/river/SAiD/output.csv" \
+    --device cpu \
+    --convert_model True \
+    --dynamic_shape True \
+    --ov_model_path "/home/openvino-ci-74/river/SAiD/dynamic_models_1" \
+    --num_steps 1
+
+   # --device gpu.1 \
+   # --use_ov True \
+   # --convert_model True \
+   # --dynamic_shape True \
+
diff -uprN SAiD/runme.sh SAiD_river/SAiD/runme.sh
--- SAiD/runme.sh	1970-01-01 08:00:00.000000000 +0800
+++ SAiD_river/SAiD/runme.sh	2025-06-17 12:55:28.094225049 +0800
@@ -0,0 +1,14 @@
+export HF_ENDPOINT=https://hf-mirror.com
+
+python3 script/inference.py \
+    --weights_path "/home/openvino-ci-74/river/SAiD/SAiD/model/SAiD.pth" \
+    --audio_path "/home/openvino-ci-74/river/SAiD/ads.wav" \
+    --output_path "/home/openvino-ci-74/river/SAiD/output_torch.csv" \
+    --device gpu.1 \
+    --use_ov True \
+    --ov_model_path "/home/openvino-ci-74/river/SAiD/dynamic_models_1" \
+    --num_steps 200
+
+   # --device gpu.1 \
+   # --use_ov True \
+   # --convert_model True \
diff -uprN SAiD/said/model/diffusion.py SAiD_river/SAiD/said/model/diffusion.py
--- SAiD/said/model/diffusion.py	2025-06-17 13:01:48.273250574 +0800
+++ SAiD_river/SAiD/said/model/diffusion.py	2025-06-17 12:55:28.095225049 +0800
@@ -19,6 +19,11 @@ from transformers import (
 from .unet_1d_condition import UNet1DConditionModel
 from .wav2vec2 import ModifiedWav2Vec2Model
 
+import openvino as ov
+from openvino.tools import mo
+
+enable_log = False
+enable_dump_output = False
 
 @dataclass
 class SAIDInferenceOutput:
@@ -58,6 +63,11 @@ class SAID(ABC, nn.Module):
         diffusion_steps: int = 1000,
         latent_scale: float = 1,
         prediction_type: str = "epsilon",
+        use_ov: bool = True,
+        ov_model_path: str = "./",
+        device_name: str = "GPU",
+        convert_model: bool = False,
+        dynamic_shape: bool = False,
     ):
         """Constructor of SAID_UNet1D
 
@@ -86,7 +96,37 @@ class SAID(ABC, nn.Module):
         self.audio_config = (
             audio_config if audio_config is not None else Wav2Vec2Config()
         )
-        self.audio_encoder = ModifiedWav2Vec2Model(self.audio_config)
+
+        self.use_ov = use_ov
+        self.convert_model = convert_model
+        self.ov_model_path = ov_model_path
+        self.convert_unet_model = False
+        self.convert_audio_model = False
+        self.device_name = device_name
+        self.dynamic_shape = dynamic_shape
+        self.count = 1
+
+        self.bias_mask = None
+        self.batch_size = 0
+        self.x_seq_len = 0
+        self.c_seq_len = 0
+
+        if convert_model:
+            self.convert_unet_model = True
+            self.convert_audio_model = True
+        if self.use_ov == True:
+            dev_name = "CPU"#self.device_name
+            print("ov::compiled_model: ", self.ov_model_path + "/ModifiedWav2Vec2Model.onnx, device_name = ", dev_name, " ...", end="")
+            core = ov.Core()
+            self.ov_audio_encoder = core.compile_model(self.ov_model_path + "/ModifiedWav2Vec2Model.xml",dev_name, config={'CACHE_DIR': self.ov_model_path})
+            print("done")
+            print("input:",self.ov_audio_encoder.inputs)
+            print("output:",self.ov_audio_encoder.outputs)
+            print()
+            self.audio_encoder = ModifiedWav2Vec2Model(self.audio_config)
+        else:
+            self.audio_encoder = ModifiedWav2Vec2Model(self.audio_config)
+
         self.audio_processor = (
             audio_processor
             if audio_processor is not None
@@ -151,7 +191,95 @@ class SAID(ABC, nn.Module):
             batch_size = noisy_samples.shape[0]
             timesteps = timesteps.repeat(batch_size)
 
-        noise_pred = self.denoiser(noisy_samples, timesteps, audio_embedding)
+        if enable_log == True:
+            print("denoiser: ", noisy_samples.shape, timesteps, audio_embedding.shape)
+
+        # accuracy debug
+        if False:
+            load_data = True
+            if load_data == True:
+                pt_name =  self.ov_model_path + "/original_UNet1D_input_A_"+str(self.count)+"_tensor.pt"
+                noisy_samples = torch.load(pt_name)
+                #pt_name =  self.ov_model_path + "/original_UNet1D_input_B_"+str(self.count)+"_tensor.pt"
+                #timesteps = torch.load(pt_name)
+                #pt_name =  self.ov_model_path + "/original_UNet1D_input_C_"+str(self.count)+"_tensor.pt"
+                #audio_embedding = torch.load(pt_name)
+            else:
+                pt_name =  self.ov_model_path + "/original_UNet1D_input_A_"+str(self.count)+"_tensor.pt"
+                torch.save(noisy_samples, pt_name)
+                #pt_name =  self.ov_model_path + "/original_UNet1D_input_B_"+str(self.count)+"_tensor.pt"
+                #torch.save(timesteps, pt_name)
+                #pt_name =  self.ov_model_path + "/original_UNet1D_input_C_"+str(self.count)+"_tensor.pt"
+                #torch.save(audio_embedding, pt_name)
+
+        batch_size = noisy_samples.shape[0]
+        x_seq_len = noisy_samples.shape[1]
+        c_seq_len = audio_embedding.shape[1]
+        pad = 1
+
+        if self.batch_size == batch_size and self.x_seq_len == x_seq_len and self.c_seq_len == c_seq_len:
+            align_bias = self.bias_mask
+        else:
+            c_x_ratio = c_seq_len / x_seq_len
+            c_kh_size = c_x_ratio / 2 + pad
+
+            align_bias = torch.ones(
+                    batch_size, x_seq_len, c_seq_len, dtype=torch.bool, device=noisy_samples.device
+            )
+            for i in range(x_seq_len):
+                c_mid = (i + 0.5) * c_x_ratio
+                c_min = max(round(float(c_mid - c_kh_size)), 0)
+                c_max = min(round(float(c_mid + c_kh_size)), c_seq_len)
+                align_bias[:, i, c_min:c_max] = False
+            self.bias_mask = align_bias
+            self.batch_size = batch_size
+            self.x_seq_len = x_seq_len
+            self.c_seq_len = c_seq_len
+
+        if self.use_ov == True:
+            name = self.ov_denoiser.output(0)
+            noise_pred = self.ov_denoiser([noisy_samples, timesteps, audio_embedding, align_bias])[name]
+            noise_pred = torch.tensor(noise_pred)
+            if enable_dump_output:
+                pt_name =  self.ov_model_path + "/ov_"+self.device_name+"_UNet1D_output_"+str(self.count)+"_tensor.pt"
+                torch.save(noise_pred, pt_name)
+        else:
+            noise_pred = self.denoiser(noisy_samples, timesteps, audio_embedding, align_bias)
+            if enable_dump_output:
+                pt_name =  self.ov_model_path + "/original_UNet1D_output_"+str(self.count)+"_tensor.pt"
+                torch.save(noise_pred, pt_name)
+
+        self.count +=1
+        if self.convert_unet_model == True:
+            dtype_mapping = {
+                torch.float32: ov.Type.f32,
+                torch.int64: ov.Type.i64,
+                torch.float64: ov.Type.f64,
+                torch.bool: ov.Type.boolean,
+            }
+
+            dummy_inputs = (noisy_samples, timesteps, audio_embedding, align_bias)
+            input_info=[]
+            if self.dynamic_shape == False:
+                for input_tensor in dummy_inputs:
+                    shape = ov.PartialShape(input_tensor.shape)
+                    element_type = dtype_mapping[input_tensor.dtype]
+                    input_info.append((shape, element_type))
+            else:
+                input_info.append((ov.PartialShape([2,-1,32]), ov.Type.f32))
+                input_info.append((ov.PartialShape([2]), ov.Type.i64))
+                input_info.append((ov.PartialShape([2,-1,768]), ov.Type.f32))
+                input_info.append((ov.PartialShape([2,-1,-1]), ov.Type.boolean))
+
+            print("Convert UNet1DConditionModel to be IR ...", end="")
+            with torch.no_grad():    
+                ov_model = ov.convert_model(self.denoiser, example_input=dummy_inputs, input=input_info)
+                #print(self.denoiser)
+            ov.save_model(ov_model, self.ov_model_path + "/UNet1DConditionModel.xml")
+            del ov_model
+            self.convert_unet_model = False
+            print(" done")
+
         return noise_pred
 
     def pred_original_sample(
@@ -224,9 +352,63 @@ class SAID(ABC, nn.Module):
             (Batch_size, embed_seq_len, embed_size), Generated audio embedding.
             If num_frames is not None, embed_seq_len = num_frames.
         """
-        features = self.audio_encoder(waveform, num_frames=num_frames).last_hidden_state
+
+        if self.use_ov == True:
+            """
+            pt_name =  self.ov_model_path + "/ov_"+self.device_name+"_audio_encoder_input_"+str(self.count)+"_tensor.pt"
+            torch.save(waveform, pt_name)
+            """
+            features = self.ov_audio_encoder(waveform)[self.ov_audio_encoder.output(0)]
+            features = torch.tensor(features)
+            if enable_dump_output:
+                pt_name =  self.ov_model_path + "/ov_"+self.device_name+"_audio_encoder_output_"+str(self.count)+"_tensor.pt"
+                torch.save(features, pt_name)
+            
+        else:
+            """
+            pt_name =  self.ov_model_path + "/original_audio_encoder_input_"+str(self.count)+"_tensor.pt"
+            torch.save(waveform, pt_name)
+            """
+            print("audio_encoder input: ", waveform.shape, num_frames)
+            features = self.audio_encoder(waveform).last_hidden_state
+            print("audio_encoder output: shape = ", features.shape)
+            if enable_dump_output:
+                pt_name =  self.ov_model_path + "/original_audio_encoder_output_"+str(self.count)+"_tensor.pt"
+                torch.save(features, pt_name)
+            
+
+        if self.convert_audio_model == True:
+            dtype_mapping = {
+                torch.float32: ov.Type.f32,
+                torch.int64: ov.Type.i64,
+                torch.float64: ov.Type.f64,
+            }
+            dummy_inputs = (waveform)
+            input_info=[]
+            if self.dynamic_shape == False:
+                shape = ov.PartialShape(waveform.shape)
+                element_type = dtype_mapping[waveform.dtype]
+                input_info.append((shape, element_type))
+            else:
+                input_info.append((ov.PartialShape([1,-1]), ov.Type.f32))
+            print("Convert ModifiedWav2Vec2Model to be IR ...",end="")
+            with torch.no_grad():    
+                ov_model = mo.convert_model(self.audio_encoder, example_input=dummy_inputs, input=input_info, compress_to_fp16=True)
+            ov.save_model(ov_model, self.ov_model_path + "/ModifiedWav2Vec2Model.xml")
+            del ov_model
+            self.convert_audio_model = False
+            print(" done!")
+
         if self.feature_dim > 0:
             features = self.audio_proj_layer(features)
+        '''
+        if self.use_ov == True:
+            pt_name =  self.ov_model_path + "/ov_"+self.device_name+"_audio_encoder_output_A_"+str(self.count)+"_tensor.pt"
+            torch.save(features, pt_name)
+        else:
+            pt_name =  self.ov_model_path + "/original_audio_encoder_output_A"+str(self.count)+"_tensor.pt"
+            torch.save(features, pt_name)
+        '''
         return features
 
     def get_random_timesteps(self, batch_size: int) -> torch.LongTensor:
@@ -372,6 +554,15 @@ class SAID(ABC, nn.Module):
         init_latents = latents.clone()
         init_timestep = min(int(num_inference_steps * strength), num_inference_steps)
 
+        """
+        if self.use_ov == True:
+            pt_name =  self.ov_model_path + "/ov_"+self.device_name+"_latents_"+str(self.count)+"_tensor.pt"
+            torch.save(latents, pt_name)
+        else:
+            pt_name =  self.ov_model_path + "/original_latents"+str(self.count)+"_tensor.pt"
+            torch.save(latents, pt_name)
+        """
+                
         # Add additional noise
         noise = None
         if init_samples is not None:
@@ -425,7 +616,9 @@ class SAID(ABC, nn.Module):
                 latent_model_input, t
             )
 
+            #print("UNet input: input = ", latent_model_input.shape, ", t = ", t, ", audio_embedding = ", audio_embedding.shape)
             noise_pred = self.forward(latent_model_input, t, audio_embedding)
+            #print("UNet output: ", noise_pred.shape)
 
             if do_classifier_free_guidance:
                 noise_pred_uncond, noise_pred_audio = noise_pred.chunk(2)
@@ -485,6 +678,11 @@ class SAID_UNet1D(SAID):
         diffusion_steps: int = 1000,
         latent_scale: float = 1,
         prediction_type: str = "epsilon",
+        use_ov: bool = True,
+        ov_model_path: str = "./",
+        device_name: str = "GPU",
+        convert_model: bool = False,
+        dynamic_shape: bool = False,
     ):
         """Constructor of SAID_UNet1D
 
@@ -515,9 +713,23 @@ class SAID_UNet1D(SAID):
             diffusion_steps=diffusion_steps,
             latent_scale=latent_scale,
             prediction_type=prediction_type,
+            use_ov=use_ov,
+            ov_model_path= ov_model_path,
+            device_name=device_name,
+            convert_model=convert_model,
+            dynamic_shape=dynamic_shape,
         )
 
         # Denoiser
+        if use_ov == True:
+            core = ov.Core()
+            print("ov::compiled_model: ", self.ov_model_path + "/UNet1DConditionModel.xml, device_name = ", self.device_name, " ...", end="")
+            self.ov_denoiser = core.compile_model(self.ov_model_path + "/UNet1DConditionModel.xml",self.device_name, config={'CACHE_DIR': self.ov_model_path})
+            print("done")
+            print("input:",self.ov_denoiser.inputs)
+            print("output:",self.ov_denoiser.outputs)
+            print()
+
         self.denoiser = UNet1DConditionModel(
             in_channels=in_channels,
             out_channels=in_channels,
diff -uprN SAiD/said/model/ldm/attention.py SAiD_river/SAiD/said/model/ldm/attention.py
--- SAiD/said/model/ldm/attention.py	2025-06-17 13:01:48.273250574 +0800
+++ SAiD_river/SAiD/said/model/ldm/attention.py	2025-06-17 12:55:28.095225049 +0800
@@ -159,34 +159,42 @@ class BasicTransformerBlock(nn.Module):
 
         self.pad = pad
 
-    def forward(self, x, context=None):
+    def forward(self, x, context=None, bias_mask=None):
         return checkpoint(
-            self._forward, (x, context), self.parameters(), self.checkpoint
+            self._forward, (x, context, bias_mask), self.parameters(), self.checkpoint
         )
 
-    def _forward(self, x, context=None):
+    def _forward(self, x, context=None, bias_mask=None):
         x = self.attn1(self.norm1(x)) + x
 
         # Alignment bias for aligning the sequences
         align_bias = None
-        if exists(context):
-            batch_size = x.shape[0]
-            x_seq_len = x.shape[1]
-            c_seq_len = context.shape[1]
-
-            c_x_ratio = c_seq_len / x_seq_len
-            c_kh_size = c_x_ratio / 2 + self.pad
-
-            align_bias = torch.ones(
-                batch_size, x_seq_len, c_seq_len, dtype=torch.bool, device=x.device
-            )
-
-            for i in range(x_seq_len):
-                c_mid = (i + 0.5) * c_x_ratio
-                c_min = max(round(c_mid - c_kh_size), 0)
-                c_max = min(round(c_mid + c_kh_size), c_seq_len)
+        if True:
+            if exists(context):
+                batch_size = x.shape[0]
+                x_seq_len = x.shape[1]
+                c_seq_len = context.shape[1]
+
+                c_x_ratio = c_seq_len / x_seq_len
+                c_kh_size = c_x_ratio / 2 + self.pad
 
-                align_bias[:, i, c_min:c_max] = False
+                align_bias = torch.ones(
+                    batch_size, x_seq_len, c_seq_len, dtype=torch.bool, device=x.device
+                )
+                #print("BasicTransformerBlock: x_seq_len = ", x_seq_len, ", shape(x) = ", x.shape)
+                for i in range(x_seq_len):
+                    c_mid = (i + 0.5) * c_x_ratio
+                    c_min = max(round(float(c_mid - c_kh_size)), 0)
+                    c_max = min(round(float(c_mid + c_kh_size)), c_seq_len)
+                    align_bias[:, i, c_min:c_max] = False
+            if torch.equal(bias_mask,align_bias) == False:
+                print("Compute bias is wrong")
+
+        if exists(context):
+            assert (
+                bias_mask is not None
+            ), "Fool!! You forgot to provide bias_mask..."
+            align_bias = bias_mask
 
         x = self.attn2(self.norm2(x), context=context, mask=align_bias) + x
         x = self.ff(self.norm3(x)) + x
@@ -220,7 +228,7 @@ class SpatialTransformer(nn.Module):
 
         self.proj_out = zero_module(conv_nd(1, in_channels, in_channels, 1))
 
-    def forward(self, x, context=None):
+    def forward(self, x, context=None, bias_mask=None):
         # note: if no context is given, cross-attention defaults to self-attention
         b, c, *spatial = x.shape
         x = x.reshape(b, c, -1)
@@ -228,7 +236,7 @@ class SpatialTransformer(nn.Module):
         x = self.norm(x)
         x = rearrange(x, "b c t -> b t c")
         for block in self.transformer_blocks:
-            x = block(x, context=context)
+            x = block(x, context=context, bias_mask=bias_mask)
         x = rearrange(x, "b t c -> b c t")
         x = self.proj_out(x)
         return (x + x_in).reshape(b, c, *spatial)
diff -uprN SAiD/said/model/ldm/openaimodel.py SAiD_river/SAiD/said/model/ldm/openaimodel.py
--- SAiD/said/model/ldm/openaimodel.py	2025-06-17 12:48:01.044195035 +0800
+++ SAiD_river/SAiD/said/model/ldm/openaimodel.py	2025-06-17 12:55:28.095225049 +0800
@@ -35,12 +35,12 @@ class TimestepEmbedSequential(nn.Sequent
     support it as an extra input.
     """
 
-    def forward(self, x, emb, context=None):
+    def forward(self, x, emb, context=None, bias_mask=None):
         for layer in self:
             if isinstance(layer, TimestepBlock):
                 x = layer(x, emb)
             elif isinstance(layer, SpatialTransformer):
-                x = layer(x, context)
+                x = layer(x, context, bias_mask)
             else:
                 x = layer(x)
         return x
@@ -674,7 +674,7 @@ class UNetModel(nn.Module):
                 # nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits
             )
 
-    def forward(self, x, timesteps=None, context=None, y=None, **kwargs):
+    def forward(self, x, timesteps=None, context=None, bias_mask=None, y=None, **kwargs):
         """
         Apply the model to an input batch.
         :param x: an [N x C x ...] Tensor of inputs.
@@ -696,12 +696,12 @@ class UNetModel(nn.Module):
 
         h = x.type(self.dtype)
         for module in self.input_blocks:
-            h = module(h, emb, context)
+            h = module(h, emb, context,bias_mask)
             hs.append(h)
-        h = self.middle_block(h, emb, context)
+        h = self.middle_block(h, emb, context, bias_mask)
         for module in self.output_blocks:
             h = th.cat([h, hs.pop()], dim=1)
-            h = module(h, emb, context)
+            h = module(h, emb, context,bias_mask)
         h = h.type(x.dtype)
         if self.predict_codebook_ids:
             return self.id_predictor(h)
diff -uprN SAiD/said/model/ldm/util.py SAiD_river/SAiD/said/model/ldm/util.py
--- SAiD/said/model/ldm/util.py	2025-06-17 13:01:48.273250574 +0800
+++ SAiD_river/SAiD/said/model/ldm/util.py	2025-06-17 12:55:28.095225049 +0800
@@ -24,6 +24,7 @@ def checkpoint(func, inputs, params, fla
                    explicitly take as arguments.
     :param flag: if False, disable gradient checkpointing.
     """
+    flag =False
     if flag:
         args = tuple(inputs) + tuple(params)
         return CheckpointFunction.apply(func, len(inputs), *args)
diff -uprN SAiD/said/model/unet_1d_condition.py SAiD_river/SAiD/said/model/unet_1d_condition.py
--- SAiD/said/model/unet_1d_condition.py	2025-06-17 13:01:48.273250574 +0800
+++ SAiD_river/SAiD/said/model/unet_1d_condition.py	2025-06-17 12:55:28.095225049 +0800
@@ -3,7 +3,7 @@
 import torch
 from torch import nn
 from .ldm.openaimodel import UNetModel
-
+import openvino as ov
 
 class UNet1DConditionModel(nn.Module):
     """Conditional 1D UNet model"""
@@ -53,6 +53,7 @@ class UNet1DConditionModel(nn.Module):
         sample: torch.FloatTensor,
         timestep: torch.Tensor,
         encoder_hidden_states: torch.Tensor,
+        bias_mask: torch.Tensor,
     ) -> torch.FloatTensor:
         """Denoise the input sample
 
@@ -70,8 +71,9 @@ class UNet1DConditionModel(nn.Module):
         torch.FloatTensor
             (Batch_size, sample_seq_len, channel), Predicted noise
         """
+        # print("UNetModel: sample = ", sample.shape, ", timestep = ", timestep.shape, ", encoder_hidden_states = ", encoder_hidden_states.shape)
         out = sample.transpose(1, 2)
-        out = self.model(out, timestep, encoder_hidden_states)
+        out = self.model(out, timestep, encoder_hidden_states, bias_mask)
         out = out.transpose(1, 2)
 
         return out
diff -uprN SAiD/script/inference.py SAiD_river/SAiD/script/inference.py
--- SAiD/script/inference.py	2025-06-17 13:01:48.274250574 +0800
+++ SAiD_river/SAiD/script/inference.py	2025-06-17 12:55:28.095225049 +0800
@@ -4,6 +4,10 @@ import argparse
 import os
 from diffusers import DDIMScheduler
 import torch
+import torchvision
+import openvino as ov
+import time
+
 from said.model.diffusion import SAID_UNet1D
 from said.util.audio import fit_audio_unet, load_audio
 from said.util.blendshape import (
@@ -102,7 +106,7 @@ def main():
     parser.add_argument(
         "--device",
         type=str,
-        default="cuda:0",
+        default="gpu",
         help="GPU/CPU device",
     )
     parser.add_argument(
@@ -115,6 +119,33 @@ def main():
         type=str,
         help="Path of the mask file (csv format)",
     )
+
+    parser.add_argument(
+        "--use_ov",
+        type=bool,
+        default=False,
+        help="Use OV backend.",
+    )
+    parser.add_argument(
+        "--convert_model",
+        type=bool,
+        default=False,
+        help="Convert torch model to be IR files.",
+    )
+
+    parser.add_argument(
+        "--ov_model_path",
+        type=str,
+        default="./",
+        help="The location of OV models for read and export.",
+    )
+    
+    parser.add_argument(
+        "--dynamic_shape",
+        type=bool,
+        default=False,
+        help="Convert model with dynamic shape.",
+    )
     args = parser.parse_args()
 
     weights_path = args.weights_path
@@ -136,6 +167,14 @@ def main():
     save_intermediate = args.save_intermediate
     show_process = True
 
+    use_ov = args.use_ov
+    convert_model = args.convert_model
+    ov_model_path = args.ov_model_path
+    dynamic_shape = args.dynamic_shape
+
+    if convert_model:
+        use_ov = False
+
     # Load init sample
     init_sample_path = args.init_sample_path
     init_samples = None
@@ -153,9 +192,19 @@ def main():
         noise_scheduler=DDIMScheduler,
         feature_dim=unet_feature_dim,
         prediction_type=prediction_type,
+        device_name = device.upper(),
+        ov_model_path = ov_model_path,
+        use_ov = use_ov,
+        convert_model = convert_model,
+        dynamic_shape = dynamic_shape,
     )
-    said_model.load_state_dict(torch.load(weights_path, map_location=device))
-    said_model.to(device)
+
+    if use_ov == True:
+        said_model.load_state_dict(torch.load(weights_path, map_location="cpu"))
+        said_model.to("cpu")
+    else:
+        said_model.load_state_dict(torch.load(weights_path, map_location=device))
+        said_model.to(device)
     said_model.eval()
 
     # Load data
@@ -168,9 +217,10 @@ def main():
     window_len = fit_output.window_size
 
     # Process the waveform
-    waveform_processed = said_model.process_audio(waveform).to(device)
+    waveform_processed = said_model.process_audio(waveform).to(device="cpu")
 
     # Inference
+    start_time = time.time()
     with torch.no_grad():
         output = said_model.inference(
             waveform_processed=waveform_processed,
@@ -184,6 +234,10 @@ def main():
             save_intermediate=save_intermediate,
             show_process=show_process,
         )
+    end_time = time.time()  # Record the end time
+    elapsed_time = (end_time - start_time) * 1000.0  # Calculate the elapsed time
+    print(f"Steps = {num_steps}, device = {device.upper()}")
+    print(f"Inference latency: {elapsed_time} ms")
 
     result = output.result[0, :window_len].cpu().numpy()
 
